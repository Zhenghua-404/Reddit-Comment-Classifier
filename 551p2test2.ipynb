{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"551p2test2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rm4RJ6SqZuU3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"5c110612-d8ff-4192-db26-80187e841dee","executionInfo":{"status":"ok","timestamp":1571280289374,"user_tz":240,"elapsed":5870,"user":{"displayName":"Ranger Teleri","photoUrl":"","userId":"05159650977050661186"}}},"source":["import numpy as np\n","import csv\n","import pandas as pd\n","import keras\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eZERps-mbLf1","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NgcVBtJbQoT","colab_type":"code","colab":{}},"source":["downloaded = drive.CreateFile({'id':'1w3nf2dE-fXLqReeIe3El99GgzsGr5Ecw'}) \n","downloaded.GetContentFile('reddit_train.csv')    \n","\n","reddit_train = pd.read_csv('reddit_train.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-8UodY2bUnb","colab_type":"code","colab":{}},"source":["def dataCompress(rawData):\n","  return rawData[:30000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcF1LbbIi2il","colab_type":"code","colab":{}},"source":["def encodeType(reddit_train):\n","  typeList = list()\n","  \n","  # extract subreddits\n","  for i in range(0,len(reddit_train)):  \n","    subreddit = reddit_train.iloc[i][2]\n","    #subreddit = reddit_train[i][2] \n","    if subreddit not in typeList:\n","      typeList.append(subreddit)    \n","    # if(len(typeList)>=20):\n","    #   break;\n","    \n","  # index subreddits  \n","  encoded = list()\n","  index = 0;\n","  for element in typeList:\n","    my_tuple = (index,element)\n","    index = index+1\n","    encoded.append(my_tuple)\n","    \n","  return encoded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7lc5kjqi5Ra","colab_type":"code","colab":{}},"source":["def typeSubreddit(reddit_train,subreddit_indexed):\n","  \n","  typeVector = list()\n","  \n","  for i in range(0,len(reddit_train)):  \n","    subreddit = reddit_train.iloc[i][2]\n","    \n","    for k in range(0,len(subreddit_indexed)):\n","      if subreddit_indexed[k][1] == subreddit:\n","        typeVector.append(subreddit_indexed[k][0])\n","   \n","  return typeVector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NJqvhIsbXoL","colab_type":"code","colab":{}},"source":["#reddit_train = dataCompress(reddit_train)\n","redditWords = reddit_train.T.iloc[1].to_numpy()\n","\n","subreddit_indexed = encodeType(reddit_train)\n","redditLabel = typeSubreddit(reddit_train,subreddit_indexed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhQ9j_F4PdQn","colab_type":"code","outputId":"ee2205cd-627d-41f7-a804-dfe1c8e7ead0","executionInfo":{"status":"ok","timestamp":1571280518695,"user_tz":240,"elapsed":7426,"user":{"displayName":"Ranger Teleri","photoUrl":"","userId":"05159650977050661186"}},"colab":{"base_uri":"https://localhost:8080/","height":843}},"source":["import nltk\n","nltk.download('popular')\n","from nltk.corpus import stopwords\n","from nltk import WordNetLemmatizer\n","from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","\n","def lemmatize_all(sentence):\n","    wnl = WordNetLemmatizer()\n","    for word, tag in pos_tag(word_tokenize(sentence)):\n","        if tag.startswith(\"NN\"):\n","            yield wnl.lemmatize(word, pos='n')\n","        elif tag.startswith('VB'):\n","            yield wnl.lemmatize(word, pos='v')\n","        elif tag.startswith('JJ'):\n","            yield wnl.lemmatize(word, pos='a')\n","        elif tag.startswith('R'):\n","            yield wnl.lemmatize(word, pos='r')\n","            \n","        else:\n","            yield word\n","\n","\n","def msgProcessing(raw_msg):\n","    m_w=[]\n","    words2=[]\n","    raw_msg=str(raw_msg)\n","    raw_msg = str(raw_msg.lower())\n","    #url_stripper= re.sub(r'Email me.*[A-Z]',\"\",s)\n","    \n","    #raw_msg=re.sub(r'\\w*[0-9]\\w*','', url_stripper)\n","    raw_msg=re.sub(r'[^a-zA-Z]', ' ', raw_msg)\n","    \n","    words=raw_msg.lower().split()\n","    #Remove words with length lesser than 3 if not w in stops\n","    for i in words:\n","        if len(i)>=1:\n","            words2.append(i)\n","    stops=set(stopwords.words('english'))\n","    m_w=\" \".join([w for w in words2])\n","    return(\" \".join(lemmatize_all(m_w)))\n","\n","\n","def helperFunction(df):\n","    print (\"Data Preprocessing!!!\")\n","    \n","    num_msg=df.shape[0]\n","    clean_msg=[]\n","    for i in range(0,num_msg):\n","        clean_msg.append(msgProcessing(df[i]))\n","    X=clean_msg\n","    print (\"Data Preprocessing Ends!!!\")\n","    return X"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OVxWi6unYOJ_","colab_type":"code","outputId":"4cac0398-f733-453c-fa1e-e3f6eb7ba9ce","executionInfo":{"status":"ok","timestamp":1571280717600,"user_tz":240,"elapsed":193533,"user":{"displayName":"Ranger Teleri","photoUrl":"","userId":"05159650977050661186"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["print(redditWords[5])\n","redditWords = helperFunction(redditWords)\n","print(redditWords[5])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The joke is on YOU!\n","\n","I've only seen it twice... :/\n","Data Preprocessing!!!\n","Data Preprocessing Ends!!!\n","the joke be on you i ve only see it twice\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L_O1wUkPbbiT","colab_type":"code","outputId":"a9a1f28e-6621-4d26-a74f-b75197a4b919","executionInfo":{"status":"ok","timestamp":1571280728718,"user_tz":240,"elapsed":5730,"user":{"displayName":"Ranger Teleri","photoUrl":"","userId":"05159650977050661186"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# dataset = pd.read_csv('sentiment_analysis/data_train.csv', sep='\\t',names=['ID', 'type', 'review', 'label']).astype(str)\n","# cw = lambda x: list(jieba.cut(x))\n","# dataset['words'] = dataset['review'].apply(cw)\n","tokenizer=Tokenizer()  #创建一个Tokenizer对象\n","#fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小\n","x_train, x_test, y_train, y_test = train_test_split(redditWords, redditLabel, test_size=0.1)\n","tokenizer.fit_on_texts(x_train)\n","vocab=tokenizer.word_index #得到每个词的编号\n","print(len(vocab))\n","\n","# 将每个样本中的每个词转换为数字列表，使用每个词的编号进行编号\n","x_train_word_ids=tokenizer.texts_to_sequences(x_train)\n","x_test_word_ids = tokenizer.texts_to_sequences(x_test)\n","#序列模式\n","# 每条样本长度不唯一，将每条样本的长度设置一个固定值\n","x_train_padded_seqs=pad_sequences(x_train_word_ids,maxlen=150) #将超过固定值的部分截掉，不足的在最前面用0填充\n","x_test_padded_seqs=pad_sequences(x_test_word_ids, maxlen=150)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["56327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K3zgzfQGd3Gl","colab_type":"code","colab":{}},"source":["from sklearn import metrics\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, BatchNormalization\n","def CNN_model(x_train_padded_seqs, y_train, x_test_padded_seqs, y_test):\n","    model = Sequential()\n","    model.add(Embedding(len(vocab) + 1, 300, input_length=150)) #使用Embeeding层将每个词编码转换为词向量\n","    model.add(Conv1D(256, 5, padding='same'))\n","    model.add(MaxPooling1D(3, 3, padding='same'))\n","    model.add(Conv1D(128, 5, padding='same'))\n","    model.add(MaxPooling1D(3, 3, padding='same'))\n","    model.add(Conv1D(64, 3, padding='same'))\n","    model.add(Flatten())\n","    model.add(Dropout(0.1))\n","    model.add(BatchNormalization())  # (批)规范化层\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(20, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","    one_hot_labels = keras.utils.to_categorical(y_train)  # 将标签转换为one-hot编码\n","    one_hot_labels_test = keras.utils.to_categorical(y_test)\n","    model.fit(x_train_padded_seqs, one_hot_labels,epochs=18, batch_size=16, validation_data=(x_test_padded_seqs, one_hot_labels_test))\n","    # y_predict = model.predict_classes(x_test_padded_seqs)  # 预测的是类别，结果就是类别号\n","    # y_predict = list(map(int, y_predict))\n","    # print('准确率', metrics.accuracy_score(y_test, y_predict))\n","    # print(y_test)\n","    # print(y_predict)\n","    #print('平均f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHsrSq-mYwQZ","colab_type":"code","colab":{}},"source":["from sklearn import metrics\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, BatchNormalization, concatenate\n","def TextCNN_model_1(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","    main_input = Input(shape=(150,), dtype='float64')\n","    # 词嵌入（使用预训练的词向量）\n","    embedder = Embedding(len(vocab) + 1, 300, trainable=False)\n","    embed = embedder(main_input)\n","    # 词窗大小分别为3,4,5\n","    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n","    cnn1 = MaxPooling1D(pool_size=48)(cnn1)\n","    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n","    cnn2 = MaxPooling1D(pool_size=47)(cnn2)\n","    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n","    cnn3 = MaxPooling1D(pool_size=46)(cnn3)\n","    # 合并三个模型的输出向量\n","    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n","    flat = Flatten()(cnn)\n","    drop = Dropout(0.2)(flat)\n","    main_output = Dense(20, activation='softmax')(drop)\n","    model = Model(inputs=main_input, outputs=main_output)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"," \n","    one_hot_labels = keras.utils.to_categorical(y_train)  # 将标签转换为one-hot编码\n","    one_hot_labels_test = keras.utils.to_categorical(y_test)\n","    model.fit(x_train_padded_seqs, one_hot_labels, batch_size=64, epochs=16, validation_data=(x_test_padded_seqs, one_hot_labels_test))\n","    #xxxxxxy_test_onehot = keras.utils.to_categorical(y_test, num_classes=3)  # 将标签转换为one-hot编码\n","    #result = model.predict(x_test_padded_seqs)  # 预测样本属于每个类别的概率\n","    #result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n","    #y_predict = list(map(int, result_labels))\n","    #print('准确率', metrics.accuracy_score(y_test, y_predict))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MufEx506i5-N","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, concatenate\n","\n","def MLP(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  model = Sequential()\n","  # 全连接层\n","  model.add(Dense(512, activation='relu'))\n","  # DropOut层\n","  model.add(Dropout(0.5))\n","  # 全连接层+分类器\n","  model.add(Dense(20, activation='softmax'))\n","\n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=100, epochs=15, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Cgac17xniFV","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, concatenate\n","\n","def LSTMmodel(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  model = Sequential()\n","  model.add(Embedding(len(vocab)+1, 300))\n","  model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.1))\n","  model.add(Dense(20, activation='softmax'))\n","\n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=64, epochs=15, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUW2tHW4CeNc","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras import regularizers\n","from keras.engine.input_layer import Input\n","from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Embedding, Bidirectional, GRU, concatenate\n","\n","def BiGRU(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  model = Sequential()\n","  model.add(Embedding(len(vocab)+1, 300))\n","  model.add(Bidirectional(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1, return_sequences=True)))\n","  model.add(Bidirectional(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1)))\n","  model.add(Dense(20, activation='softmax'))\n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=32, epochs=15, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvGTJIZkjBxQ","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import GRU, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, concatenate, Bidirectional\n","\n","def CNNRNN(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  main_input = Input(shape=(150,), dtype='float64')\n","  embed = Embedding(len(vocab)+1, 300)(main_input)\n","  cnn = Conv1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n","  cnn = MaxPooling1D(pool_size=4)(cnn)\n","  cnn = Flatten()(cnn)\n","  cnn = Dense(256)(cnn)\n","  rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n","  rnn = Dense(256)(rnn)\n","  con = concatenate([cnn,rnn], axis=-1)\n","  main_output = Dense(20, activation='softmax')(con)\n","  model = Model(inputs = main_input, outputs = main_output)\n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=32, epochs=10, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZwCBmc6Yolg","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras import regularizers\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import GRU, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, concatenate, BatchNormalization\n","\n","def CNNLSTM(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  model = Sequential()\n","  model.add(Embedding(len(vocab)+1, 300, input_length=150))\n","  model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n","  model.add(MaxPooling1D(pool_size=3))\n","  model.add(Dropout(0.3))\n","  model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n","  model.add(MaxPooling1D(pool_size=3))\n","  model.add(Dropout(0.35))\n","  model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n","  model.add(MaxPooling1D(pool_size=3))\n","  model.add(Dropout(0.4))\n","  model.add(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1, return_sequences = True))\n","  model.add(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1))\n","  model.add(Dropout(0.25))\n","  #model.summary()\n","  #model.add(Flatten())\n","  model.add(Dense(128,activation='relu'))\n","  model.add(Dropout(0.45))\n","  model.add(Dense(20, activation='softmax'))\n","  \n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=128, epochs=20, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRS0yJ0OSl7j","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.engine.input_layer import Input\n","from keras.layers import GRU, Activation, Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D\n","from keras.layers import concatenate, Bidirectional, BatchNormalization\n","from keras.utils import plot_model\n","#from google.colab import files\n","\n","def Trinity(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n","  main_input = Input(shape=(150,), dtype='float64')\n","  embed = Embedding(len(vocab)+1, 300)(main_input)\n","\n","  cnnlstm = Conv1D(32,kernel_size=3,padding='same',activation='relu')(embed)\n","  cnnlstm = MaxPooling1D(pool_size=3)(cnnlstm)\n","  cnnlstm = Dropout(0.3)(cnnlstm)\n","  cnnlstm = Conv1D(64,kernel_size=3,padding='same',activation='relu')(cnnlstm)\n","  cnnlstm = MaxPooling1D(pool_size=3)(cnnlstm)\n","  cnnlstm = Dropout(0.35)(cnnlstm)\n","  cnnlstm = Conv1D(128,kernel_size=3,padding='same',activation='relu')(cnnlstm)\n","  cnnlstm = MaxPooling1D(pool_size=3)(cnnlstm)\n","  cnnlstm = Dropout(0.4)(cnnlstm)\n","  cnnlstm = GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences = True)(cnnlstm)\n","  cnnlstm = GRU(256, dropout=0.2, recurrent_dropout=0.1, go_backwards=True)(cnnlstm)\n","  cnnlstm = Dropout(0.25)(cnnlstm)\n","  cnnlstm = Dense(128)(cnnlstm)\n","  \n","  cnn = Conv1D(256, 5, padding='same')(embed)\n","  cnn = MaxPooling1D(3, 3, padding='same')(cnn)\n","  cnn = Conv1D(128, 5, padding='same')(cnn)\n","  cnn = MaxPooling1D(3, 3, padding='same')(cnn)\n","  cnn = Conv1D(64, 3, padding='same')(cnn)\n","  cnn = Flatten()(cnn)\n","  cnn = Dropout(0.1)(cnn)\n","  cnn = BatchNormalization()(cnn)\n","  cnn = Dense(128)(cnn)\n","\n","\n","  bigru = Bidirectional(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1, return_sequences=True))(embed)\n","  bigru = Bidirectional(GRU(256, kernel_initializer='orthogonal', recurrent_regularizer=regularizers.l2(0.01), dropout=0.2, recurrent_dropout=0.1))(bigru)\n","  bigru = Dense(128)(bigru)\n","\n","  \n","  con = concatenate([cnnlstm,cnn,bigru], axis=-1)\n","  main_output = Dense(20, activation='softmax')(con)\n","  model = Model(inputs = main_input, outputs = main_output)\n","  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","  # model.summary()\n","  # plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n","  # uploaded = drive.CreateFile({'title': 'abc.png'})\n","  # uploaded.SetContentFile('model.png')\n","  # uploaded.Upload() \n","\n","  one_hot_labels_train = keras.utils.to_categorical(y_train)\n","  one_hot_labels_test = keras.utils.to_categorical(y_test)\n","  model.fit(x_train_padded_seqs, one_hot_labels_train, batch_size=64, epochs=15, validation_data=(x_test_padded_seqs, one_hot_labels_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ff5ZVcRveCM7","colab_type":"code","outputId":"0918f2dd-2a28-405d-ae18-41700c39092f","executionInfo":{"status":"ok","timestamp":1571283744996,"user_tz":240,"elapsed":811367,"user":{"displayName":"Ranger Teleri","photoUrl":"","userId":"05159650977050661186"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["Trinity(x_train_padded_seqs, y_train, x_test_padded_seqs, y_test)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Train on 63000 samples, validate on 7000 samples\n","Epoch 1/1\n","63000/63000 [==============================] - 801s 13ms/step - loss: 2.5789 - acc: 0.3325 - val_loss: 1.9196 - val_acc: 0.4574\n"],"name":"stdout"}]}]}