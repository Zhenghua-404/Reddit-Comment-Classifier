{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy1 of 551p2test1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkYKbbBU47Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from datetime import datetime\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l02b6Dg454Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHB6Ag8g56Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1w3nf2dE-fXLqReeIe3El99GgzsGr5Ecw'}) \n",
        "downloaded.GetContentFile('reddit_train.csv')    \n",
        "\n",
        "reddit_train = pd.read_csv('reddit_train.csv')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1N35C1j6ufoUMKAaEC41c9NMOsKRNSsAX'})\n",
        "downloaded.GetContentFile('reddit_test.csv')    \n",
        "\n",
        "reddit_test = pd.read_csv('reddit_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shHWQRrFcdyd",
        "colab_type": "code",
        "outputId": "c0721053-f3f7-447e-f79d-253e41a684e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def lemmatize_all(sentence):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
        "        if tag.startswith(\"NN\"):\n",
        "            yield wnl.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('VB'):\n",
        "            yield wnl.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('JJ'):\n",
        "            yield wnl.lemmatize(word, pos='a')\n",
        "        elif tag.startswith('R'):\n",
        "            yield wnl.lemmatize(word, pos='r')\n",
        "            \n",
        "        else:\n",
        "            yield word\n",
        "\n",
        "\n",
        "def msgProcessing(raw_msg):\n",
        "    m_w=[]\n",
        "    words2=[]\n",
        "    raw_msg=str(raw_msg)\n",
        "    raw_msg = str(raw_msg.lower())\n",
        "    #url_stripper= re.sub(r'Email me.*[A-Z]',\"\",s)\n",
        "    \n",
        "    #raw_msg=re.sub(r'\\w*[0-9]\\w*','', url_stripper)\n",
        "    raw_msg=re.sub(r'[^a-zA-Z]', ' ', raw_msg)\n",
        "    \n",
        "    words=raw_msg.lower().split()\n",
        "    #Remove words with length lesser than 3 if not w in stops\n",
        "    for i in words:\n",
        "        if len(i)>=1:\n",
        "            words2.append(i)\n",
        "    stops=set(stopwords.words('english'))\n",
        "    m_w=\" \".join([w for w in words2])\n",
        "    return(\" \".join(lemmatize_all(m_w)))\n",
        "\n",
        "\n",
        "def helperFunction(df):\n",
        "    print (\"Data Preprocessing!!!\")\n",
        "    \n",
        "    num_msg=df.shape[0]\n",
        "    clean_msg=[]\n",
        "    for i in range(0,num_msg):\n",
        "        clean_msg.append(msgProcessing(df[i]))\n",
        "    X=clean_msg\n",
        "    print (\"Data Preprocessing Ends!!!\")\n",
        "    return X"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3RHIWAT7JE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply cross validation\n",
        "def val6v1(rawData):\n",
        "\n",
        "    test_dataset=rawData[60000:]\n",
        "    train_dataset=rawData[:60000]\n",
        "    \n",
        "    return train_dataset,test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJmBGJjL-Egc",
        "colab_type": "code",
        "outputId": "2117652c-9890-4d31-c4b0-f56ced9a5b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#vald\n",
        "redditWords = helperFunction(reddit_train.T.iloc[1].to_numpy())\n",
        "trainData, testData = val6v1(redditWords)\n",
        "trainTar, testTar = val6v1(reddit_train.T.iloc[-1].to_numpy())\n",
        "print(trainData[5])\n",
        "print(testData[5])\n",
        "print(trainTar)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessing!!!\n",
            "Data Preprocessing Ends!!!\n",
            "the joke be on you i ve only see it twice\n",
            "never cease to amaze me how gullible you people be\n",
            "['hockey' 'nba' 'leagueoflegends' ... 'nfl' 'nfl' 'nfl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4haD2rDdK2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#realTest\n",
        "# trainData = helperFunction(reddit_train.T.iloc[1].to_numpy())\n",
        "# testData = helperFunction(reddit_test.T.iloc[1].to_numpy())\n",
        "# trainTar = reddit_train.T.iloc[-1].to_numpy()\n",
        "# print(trainData[5])\n",
        "# print(testData[8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlEpK2t067TD",
        "colab_type": "code",
        "outputId": "6a5b79df-f47c-4325-f896-3aadc9e54c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = 'english')\n",
        "tfidf_train_2 = tv.fit_transform(trainData);\n",
        "tv2 = TfidfVectorizer(vocabulary = tv.vocabulary_)\n",
        "tfidf_test_2 = tv2.fit_transform(testData)\n",
        "print(\"the shape of train is \"+repr(tfidf_train_2.shape))\n",
        "print(\"the shape of test is \"+repr(tfidf_test_2.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the shape of train is (60000, 54667)\n",
            "the shape of test is (10000, 54667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKp0sZ_3Az3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "def calculate_result(actual,pred):\n",
        "    m_precision = metrics.precision_score(actual,pred,average='micro')\n",
        "    m_recall = metrics.recall_score(actual,pred,average='micro')\n",
        "    print('predict info:')\n",
        "    print('precision:{0:.3f}'.format(m_precision))\n",
        "    print('recall:{0:0.3f}'.format(m_recall))\n",
        "    print('f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred,average='micro')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQU9QWvTat2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def saveCSV(pred, modName):\n",
        "  pred = pd.DataFrame(pred, columns=['Category'])\n",
        "  pred.to_csv('predResult.csv', index=True, index_label='Id', header=True)\n",
        "  fileName = modName+''+time.strftime(\"%Y%m%d%H\", time.localtime())+'.csv'\n",
        "  uploaded = drive.CreateFile({'title': fileName})\n",
        "  uploaded.SetContentFile('predResult.csv')\n",
        "  uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tp2Pw-8dUuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4e0a6826-f188-4086-ca98-02efd2bc148f"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "current_time = datetime.now()\n",
        "Bclf = BernoulliNB()\n",
        "Bclf.fit(tfidf_train_2, trainTar)\n",
        "varA = Bclf.score(tfidf_test_2, testTar)\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(varA)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:00:00.356745\n",
            "0.5126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8CJe2p5BG0f",
        "colab_type": "code",
        "outputId": "5baedab9-01e3-4c8b-871d-9a15efb49716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#fea_test = vectorizer.fit_transform(newsgroups_test.data)\n",
        "#create the Multinomial Naive Bayesian Classifier\n",
        "\n",
        "current_time = datetime.now()\n",
        "clf = MultinomialNB(alpha = 0.32)  #0.32\n",
        "clf.fit(tfidf_train_2, trainTar)\n",
        "#pred = clf.predict(tfidf_test_2)\n",
        "#calculate_result(testTar,pred)\n",
        "varA = clf.score(tfidf_test_2, testTar)\n",
        "#saveCSV(pred, 'MultNB')\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(varA)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:00:00.317397\n",
            "0.5772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFMrfIwiVYHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a0d30a9a-8ed2-4a27-eb42-9fd2feca7c1c"
      },
      "source": [
        "# with cross-validation\n",
        "current_time = datetime.now()\n",
        "\n",
        "Cscore = cross_val_score(clf, tfidf_train_2, trainTar, cv=20)\n",
        "\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(Cscore)\n",
        "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (Cscore.mean(), Cscore.std() * 2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:00:06.614722\n",
            "[0.57693586 0.56729811 0.5525266  0.57646277 0.58064516 0.57903494\n",
            " 0.57404326 0.56472546 0.58441558 0.55814728 0.56466667 0.5730487\n",
            " 0.57223891 0.56527546 0.56260434 0.57434013 0.55967904 0.56603143\n",
            " 0.57525084 0.55351171]\n",
            "Accuracy: 0.569 (+/- 0.02)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcrgUBkOD3Xc",
        "colab_type": "code",
        "outputId": "55a62b37-fe2b-4855-ff90-7ca7f0be07c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "current_time = datetime.now()\n",
        "svclf = SVC(kernel = 'linear')#default with 'rbf' linear is best now\n",
        "svclf.fit(tfidf_train_2, trainTar)\n",
        "svclf.score(tfidf_test_2, testTar)\n",
        "#pred = svclf.predict(tfidf_test_2)\n",
        "#calculate_result(testTar,pred)\n",
        "Cscore = cross_val_score(svclf, tfidf_test_2, testTar, cv=3)\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(Cscore)\n",
        "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (Cscore.mean(), Cscore.std() * 2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:13:06.074546\n",
            "[0.39383049 0.39243924 0.38972356]\n",
            "Accuracy: 0.392 (+/- 0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgS02rHWTXu",
        "colab_type": "code",
        "outputId": "156d6eb6-5ece-44d4-856f-87590cae9da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "current_time = datetime.now()\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(tfidf_train_2, trainTar)\n",
        "#pred = clf.predict(tfidf_test_2)\n",
        "#calculate_result(testTar,pred)\n",
        "\n",
        "Cscore = cross_val_score(clf, tfidf_train_2, trainTar, cv=3)\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(Cscore)\n",
        "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (Cscore.mean(), Cscore.std() * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict info:\n",
            "precision:0.332\n",
            "recall:0.332\n",
            "f1-score:0.332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2qbsEHoXxWO",
        "colab_type": "code",
        "outputId": "a627e7d5-d078-41f2-ca16-35f3b3a2dba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "current_time = datetime.now()\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=4000).fit(tfidf_train_2, trainTar)\n",
        "#pred = clf.predict(tfidf_test_2)\n",
        "#calculate_result(testTar,pred)\n",
        "clf.score(tfidf_test_2, testTar)\n",
        "#saveCSV(pred, 'LogReg')\n",
        "\n",
        "Cscore = cross_val_score(clf, tfidf_train_2, trainTar, cv=4)\n",
        "print(\"Training took time \", datetime.now() - current_time)\n",
        "print(Cscore)\n",
        "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (Cscore.mean(), Cscore.std() * 2))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:04:34.692869\n",
            "[0.53661625 0.54336378 0.53707161 0.53974923]\n",
            "Accuracy: 0.539 (+/- 0.01)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz6emVzJPH3R",
        "colab_type": "code",
        "outputId": "aef56a4c-1f7a-4c27-94d3-b413195e9a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knnclf = KNeighborsClassifier(n_neighbors=20)#default with k=5\n",
        "knnclf.fit(tfidf_train_2, trainTar)\n",
        "pred = knnclf.predict(tfidf_test_2)\n",
        "calculate_result(testTar,pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict info:\n",
            "precision:0.067\n",
            "recall:0.067\n",
            "f1-score:0.067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD_hxGDAXdL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# gnb = GaussianNB()\n",
        "# pred = gnb.fit(tfidf_train_2.toarray(), trainTar).predict(tfidf_test_2.toarray())\n",
        "# calculate_result(testTar,pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}