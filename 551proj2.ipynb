{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"551proj2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"dAOu04vtVyKs","colab_type":"code","outputId":"162627b5-1840-4796-bf3c-1c97d966ff56","executionInfo":{"status":"ok","timestamp":1571000380215,"user_tz":240,"elapsed":241,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":817}},"source":["import numpy as np\n","import csv\n","import pandas as pd\n","import nltk\n","import random\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","nltk.download(\"popular\")\n","from scipy import sparse"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"09F5ociIZPt1","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcpWuFkgZZ1L","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fq9fV8NtZrVk","colab_type":"code","colab":{}},"source":["# file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n","# for file1 in file_list:\n","#   print('title: %s' % (file1['title']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFWno2uWYNw2","colab_type":"code","colab":{}},"source":["file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n","  \n","downloaded = drive.CreateFile({'id':'1A6SqtCXt81Lqhoph0iWNOAd0HoI_GYAW'}) \n","downloaded.GetContentFile('reddit_train.csv')    \n","\n","reddit_train = pd.read_csv('reddit_train.csv')\n","\n","# print(len(reddit_train))\n","# print(reddit_train.iloc[0][1])\n","\n","\n","# tokenize the comments\n","def loadToken(reddit_train):\n","  reddit_tokens = []\n","  for i in range(0,len(reddit_train)):\n","    entry = word_tokenize(reddit_train.iloc[i][1])\n","    print(entry)\n","    reddit_tokens.append(entry)\n","\n","  #print(len(reddit_tokens))  \n","  #sum_preclean_tokens = sum(len(entry) for entry in reddit_tokens)\n","  #print(sum_preclean_tokens)\n","  \n","  return reddit_tokens\n","#all words in comments including puntuation marks"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaLTege3OV1Q","colab_type":"code","outputId":"0628c49c-b201-43aa-c872-e13d296c6f86","executionInfo":{"status":"ok","timestamp":1570997235921,"user_tz":240,"elapsed":470,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# clean data for processing\n","def clean(entry):\n","\n","  # delete non-alphabetical tokens and lowercases\n","  tokensA = [token.lower() for token in entry if token.isalpha()]\n","  # delete stopwords\n","  stopword = set(stopwords.words('english'))\n","  #print(stopword)\n","  tokensB = [token for token in tokensA if token not in stopword]\n","\n","  # lemmatization\n","  output = [WordNetLemmatizer().lemmatize(token) for token in tokensB]\n","    \n","  return output\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tfidf']"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"gB0OGiVXXauF","colab_type":"code","outputId":"33b71c16-cc47-46e5-ab5b-9f85d09db9e9","executionInfo":{"status":"ok","timestamp":1570988489566,"user_tz":240,"elapsed":74720,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["pre_tokens = loadToken(reddit_train)\n","\n","cleaned_tokens = [clean(entry) for entry in pre_tokens]\n","\n","calSum = sum([sum(len(x) for x in cleaned_tokens)])\n","print(calSum)  # total number of tokens: 1452983\n","\n","print(len(cleaned_tokens))\n","\n","print(cleaned_tokens[0])\n","#print(cleaned_tokens[69999])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1452983\n","70000\n","['honestly', 'buffalo', 'correct', 'answer', 'remember', 'people', 'somewhat', 'joking', 'buffalo', 'mantra', 'starting', 'goalie', 'win', 'game', 'get', 'traded', 'think', 'edmonton', 'front', 'office', 'travesty', 'better', 'part', 'year', 'buffalo', 'systematic', 'destruction', 'term', 'much', 'responsible', 'change', 'draft', 'lottery']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hs1E1HncDJVd","colab_type":"code","colab":{}},"source":["#partition dataset to train and test in a ratio(testPercent)\n","def partitionData(cleanedData, testPercent):\n","    howManyNumbers = int(round(testPercent*len(cleanedData)))\n","    #shuffled = cleanedData[:]\n","    #random.shuffle(shuffled)\n","    test_dataset=cleanedData[howManyNumbers:]\n","    train_dataset=cleanedData[:howManyNumbers]\n","    return train_dataset,test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XL0Zwb-8mwLe","colab_type":"code","colab":{}},"source":["# create a word bank (removed repeated words) (only for training data)\n","def wordBankCreator(cleaned_tokens):\n","\n","  ResultList = list()\n","  for entry in cleaned_tokens:\n","    for token in entry:\n","      if token not in ResultList:\n","        ResultList.append(token)\n","        \n","  return ResultList     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfasH27utLLg","colab_type":"code","colab":{}},"source":["# extract a list if subreddit categories and match each with number index\n","def encodeType(reddit_train):\n","  typeList = list()\n","  \n","  # extract subreddits\n","  for i in range(0,len(reddit_train)):  \n","    subreddit = reddit_train.iloc[i][2]\n","    #subreddit = reddit_train[i][2] \n","    if subreddit not in typeList:\n","      typeList.append(subreddit)    \n","    # if(len(typeList)>=20):\n","    #   break;\n","    \n","  # index subreddits  \n","  encoded = list()\n","  index = 0;\n","  for element in typeList:\n","    my_tuple = (index,element)\n","    index = index+1\n","    encoded.append(my_tuple)\n","    \n","  return encoded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YrxMsPB4GNJ","colab_type":"code","colab":{}},"source":["# return vector that indicates subreddit category\n","def typeSubreddit(reddit_train,subreddit_indexed):\n","  \n","  typeVector = list()\n","  \n","  for i in range(0,len(reddit_train)):  \n","    subreddit = reddit_train.iloc[i][2]\n","    \n","    for k in range(0,len(subreddit_indexed)):\n","      if subreddit_indexed[k][1] == subreddit:\n","        typeVector.append(subreddit_indexed[k][0])\n","   \n","  return typeVector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3HGKfHY6yu5","colab_type":"code","outputId":"423b1ae7-2c4c-4724-d6d2-0185bfad1e2d","executionInfo":{"status":"ok","timestamp":1570988736591,"user_tz":240,"elapsed":230190,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["train_dataset,test_dataset=partitionData(cleaned_tokens,0.9)\n","wordBank = wordBankCreator(train_dataset)\n","print(len(wordBank))   # total number in word bank: 55145 \n","\n","subreddit_indexed = encodeType(reddit_train)\n","print(subreddit_indexed)  # correctly gives 20\n","\n","subreddit_type = typeSubreddit(reddit_train,subreddit_indexed)\n","train_type,test_type=partitionData(subreddit_type,0.9)\n","print(len(train_type))   # correctly gives 70000   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["52410\n","[(0, 'hockey'), (1, 'nba'), (2, 'leagueoflegends'), (3, 'soccer'), (4, 'funny'), (5, 'movies'), (6, 'anime'), (7, 'Overwatch'), (8, 'trees'), (9, 'GlobalOffensive'), (10, 'nfl'), (11, 'AskReddit'), (12, 'gameofthrones'), (13, 'conspiracy'), (14, 'worldnews'), (15, 'wow'), (16, 'europe'), (17, 'canada'), (18, 'Music'), (19, 'baseball')]\n","63000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-W47WKkRArt","colab_type":"code","outputId":"6c31f242-87ae-4ccd-8c04-09968a73524a","executionInfo":{"status":"ok","timestamp":1570990113392,"user_tz":240,"elapsed":14,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["# return X matrix: for each comment, represent occurance of each word in wordBank with 1 or 0\n","# outcome expected - sparse matrix: 70000 * 55145\n","def BernoulliMatrix(cleaned_tokens,wordBank):\n","\n","  matrixResult = sparse.lil_matrix((len(cleaned_tokens),len(wordBank)),dtype=np.int8)\n","  \n","  print(matrixResult.shape)\n","  print(len(cleaned_tokens))\n","  print(len(wordBank))      \n","  \n","  for i in range(0,len(cleaned_tokens)):\n","    \n","    comment_cleaned = cleaned_tokens[i]\n","\n","    for j in range(0,len(comment_cleaned)):\n","      \n","      if comment_cleaned[j] in wordBank:\n","        occurIndex = wordBank.index(comment_cleaned[j])\n","        matrixResult[i,occurIndex] = 1\n","      \n","    \n","  return matrixResult\n","    \n","    \n","    \n","train_matrixResult = BernoulliMatrix(train_dataset,wordBank)\n","test_matrixResult = BernoulliMatrix(test_dataset,wordBank)\n","#print(matrixResult.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(63000, 52410)\n","63000\n","52410\n","(7000, 52410)\n","7000\n","52410\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"286hq_69J0qp","colab_type":"code","colab":{}},"source":[" #Bernoulli Naive Bayes\n","\n","import numpy as np\n","\n","class BernNB() :\n","  \n","  def __init__(self):\n","    c = 20\n","    \n","\n","  def fit(self, X, Y, c):\n","    m = X.shape[1]\n","    pY1 = np.zeros((c,), dtype = np.float_)\n","    xJy = np.zeros((c, m), dtype = np.float_)\n","    n = X.shape[0]\n","    \n","    for i in range(0, n):\n","      numClass = Y[i]\n","      pY1[numClass] = pY1[numClass] + 1\n","    \n","    print(\"000000000000\")\n","\n","    for j in range(0, n):\n","      for k in range(0, m):\n","        if X[j, k] == 1:\n","          xJy[Y[j], k] = xJy[Y[j], k] + 1\n","    \n","    print(\"000000000000\")\n","\n","    for i in range(0, c):\n","      yAdd2 = pY1[i] + 2\n","      for j in range(0, m):\n","        xJy[i, j] = (xJy[i, j] + 1)/yAdd2\n","    \n","    pY1 * (1/n)\n","    self.pY1 = pY1\n","    self.xJy = xJy\n","\n","\n","  def predict(self, X):\n","    n = X.shape[0]\n","    m = X.shape[1]\n","    c = self.xJy.shape[0]\n","    totRes = np.zeros((n,))\n","\n","    for i in range(0, n):\n","      maxY = 0\n","      maxProb = 0\n","      for j in range(0, c):\n","        prob = self.pY1[j]\n","        for k in range(0, m):\n","          if X[i, k] == 1:\n","            prob = prob * self.xJy[j, k]\n","          else:\n","            prob = prob * (1 - self.xJy[j, k])\n","        if prob > maxProb:\n","          maxY = j\n","          maxProb = prob\n","      totRes[i] = maxY\n","    \n","    return totRes\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpOa_7HfZHEM","colab_type":"code","outputId":"8e560a4e-91bb-4eac-fc66-15b55eb0f54f","executionInfo":{"status":"ok","timestamp":1570791544578,"user_tz":240,"elapsed":12808715,"user":{"displayName":"Mavis Chen","photoUrl":"","userId":"12085087395179740336"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["BNB = BernNB()\n","BNB.fit(train_matrixResult.toarray(),train_type,20)\n","result = BNB.predict(test_matrixResult.toarray())\n","\n","if result.shape[0] != len(test_type):\n","  print(\"出问题了\")\n","else:\n","  accu = 0\n","  for i in range(0, result.shape[0]):\n","    if result[i] == test_type[i]:\n","      accu = accu + 1\n","  \n","  print(accu/result.shape[0])\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["000000000000\n","000000000000\n","0.5034285714285714\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iilUGc-zh-Gz","colab_type":"text"},"source":["5w of wordBank is too big;\n","\n","the predicted output should be stored back in test dataset (number->category)"]}]}